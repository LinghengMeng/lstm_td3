{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pybulletgym\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from spinup.utils.logx import EpochLogger\n",
    "import itertools\n",
    "from spinup.env_wrapper.pomdp_wrapper import POMDPWrapper\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # \"cuda\"\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, max_size):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.max_size = max_size\n",
    "        self.obs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.ptr, self.size = 0, 0\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.obs2_buf[self.ptr] = list(next_obs)\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in batch.items()}\n",
    "\n",
    "    def sample_batch_with_history(self, batch_size=32, max_hist_len=100):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch_size:\n",
    "        :param max_hist_len: the length of experiences before current experience\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        idxs = np.random.randint(max_hist_len, self.size, size=batch_size)\n",
    "        # History\n",
    "        if max_hist_len == 0:\n",
    "            hist_obs = np.zeros([batch_size, 1, self.obs_dim])\n",
    "            hist_act = np.zeros([batch_size, 1, self.act_dim])\n",
    "            hist_obs2 = np.zeros([batch_size, 1, self.obs_dim])\n",
    "            hist_act2 = np.zeros([batch_size, 1, self.act_dim])\n",
    "            hist_rew = np.zeros([batch_size, 1])\n",
    "            hist_done = np.zeros([batch_size, 1])\n",
    "            hist_len = np.zeros(batch_size)\n",
    "        else:\n",
    "            hist_obs = np.zeros([batch_size, max_hist_len, self.obs_dim])\n",
    "            hist_act = np.zeros([batch_size, max_hist_len, self.act_dim])\n",
    "            hist_obs2 = np.zeros([batch_size, max_hist_len, self.obs_dim])\n",
    "            hist_act2 = np.zeros([batch_size, max_hist_len, self.act_dim])\n",
    "            hist_rew = np.zeros([batch_size, max_hist_len])\n",
    "            hist_done = np.zeros([batch_size, max_hist_len])\n",
    "            hist_len = max_hist_len * np.ones(batch_size)\n",
    "            # Extract history experiences before sampled index\n",
    "            for hist_i in range(max_hist_len):\n",
    "                hist_obs[:, -1 - hist_i, :] = self.obs_buf[idxs - hist_i - 1, :]\n",
    "                hist_act[:, -1 - hist_i, :] = self.act_buf[idxs - hist_i - 1, :]\n",
    "                hist_obs2[:, -1 - hist_i, :] = self.obs2_buf[idxs - hist_i - 1, :]\n",
    "                hist_act2[:, -1 - hist_i, :] = self.act_buf[idxs - hist_i, :]  # include a_t\n",
    "                hist_rew[:, -1 - hist_i] = self.rew_buf[idxs - hist_i - 1]\n",
    "                hist_done[:, -1 - hist_i] = self.done_buf[idxs - hist_i - 1]\n",
    "            # If there is done in the backward experiences, only consider the experiences after the last done.\n",
    "            for batch_i in range(batch_size):\n",
    "                done_idxs_exclude_last_exp = np.where(hist_done[batch_i][:-1] == 1)  # Exclude last experience\n",
    "                # If exist done\n",
    "                if done_idxs_exclude_last_exp[0].size != 0:\n",
    "                    largest_done_id = done_idxs_exclude_last_exp[0][-1]\n",
    "                    hist_len[batch_i] = max_hist_len - (largest_done_id + 1)\n",
    "\n",
    "                    # Only keep experiences after the last done\n",
    "                    obs_keep_part = np.copy(hist_obs[batch_i, largest_done_id + 1:, :])\n",
    "                    act_keep_part = np.copy(hist_act[batch_i, largest_done_id + 1:, :])\n",
    "                    obs2_keep_part = np.copy(hist_obs2[batch_i, largest_done_id + 1:, :])\n",
    "                    act2_keep_part = np.copy(hist_act2[batch_i, largest_done_id + 1:, :])\n",
    "                    rew_keep_part = np.copy(hist_rew[batch_i, largest_done_id + 1:])\n",
    "                    done_keep_part = np.copy(hist_done[batch_i, largest_done_id + 1:])\n",
    "\n",
    "                    # Set to 0 to make sure all experiences are at the beginning\n",
    "                    hist_obs[batch_i] = np.zeros([max_hist_len, self.obs_dim])\n",
    "                    hist_act[batch_i] = np.zeros([max_hist_len, self.act_dim])\n",
    "                    hist_obs2[batch_i] = np.zeros([max_hist_len, self.obs_dim])\n",
    "                    hist_act2[batch_i] = np.zeros([max_hist_len, self.act_dim])\n",
    "                    hist_rew[batch_i] = np.zeros([max_hist_len])\n",
    "                    hist_done[batch_i] = np.zeros([max_hist_len])\n",
    "\n",
    "                    # Move kept experiences to the start of the segment\n",
    "                    hist_obs[batch_i, :max_hist_len - (largest_done_id + 1), :] = obs_keep_part\n",
    "                    hist_act[batch_i, :max_hist_len - (largest_done_id + 1), :] = act_keep_part\n",
    "                    hist_obs2[batch_i, :max_hist_len - (largest_done_id + 1), :] = obs2_keep_part\n",
    "                    hist_act2[batch_i, :max_hist_len - (largest_done_id + 1), :] = act2_keep_part\n",
    "                    hist_rew[batch_i, :max_hist_len - (largest_done_id + 1)] = rew_keep_part\n",
    "                    hist_done[batch_i, :max_hist_len - (largest_done_id + 1)] = done_keep_part\n",
    "        #\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs],\n",
    "                     hist_obs=hist_obs,\n",
    "                     hist_act=hist_act,\n",
    "                     hist_obs2=hist_obs2,\n",
    "                     hist_act2=hist_act2,\n",
    "                     hist_rew=hist_rew,\n",
    "                     hist_done=hist_done,\n",
    "                     hist_len=hist_len)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim,\n",
    "                 mem_pre_lstm_hid_sizes=(128,),\n",
    "                 mem_lstm_hid_sizes=(128,),\n",
    "                 mem_after_lstm_hid_size=(128,),\n",
    "                 cur_feature_hid_sizes=(128,),\n",
    "                 post_comb_hid_sizes=(128,),\n",
    "                 mem_gate=True,\n",
    "                 mem_gate_before_current_feature_extraction=False,\n",
    "                 hist_with_past_act=False, use_hist_mask=False):\n",
    "        super(MLPCritic, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.mem_gate = mem_gate\n",
    "        self.mem_gate_before_current_feature_extraction = mem_gate_before_current_feature_extraction\n",
    "        self.hist_with_past_act = hist_with_past_act\n",
    "        self.use_hist_mask = use_hist_mask\n",
    "        #\n",
    "        self.mem_pre_lstm_layers = nn.ModuleList()\n",
    "        self.mem_lstm_layers = nn.ModuleList()\n",
    "        self.mem_after_lstm_layers = nn.ModuleList()\n",
    "\n",
    "        self.mem_gate_layer = nn.ModuleList()\n",
    "\n",
    "        self.cur_feature_layers = nn.ModuleList()\n",
    "        self.post_combined_layers = nn.ModuleList()\n",
    "        # Memory\n",
    "        #    Pre-LSTM\n",
    "        if self.hist_with_past_act:\n",
    "            mem_pre_lstm_layer_size = [obs_dim + act_dim] + list(mem_pre_lstm_hid_sizes)\n",
    "        else:\n",
    "            mem_pre_lstm_layer_size = [obs_dim] + list(mem_pre_lstm_hid_sizes)\n",
    "        for h in range(len(mem_pre_lstm_layer_size) - 1):\n",
    "            self.mem_pre_lstm_layers += [nn.Linear(mem_pre_lstm_layer_size[h],\n",
    "                                                   mem_pre_lstm_layer_size[h + 1]),\n",
    "                                         nn.ReLU()]\n",
    "        #    LSTM\n",
    "        self.mem_lstm_layer_sizes = [mem_pre_lstm_layer_size[-1]] + list(mem_lstm_hid_sizes)\n",
    "        for h in range(len(self.mem_lstm_layer_sizes) - 1):\n",
    "            self.mem_lstm_layers += [\n",
    "                nn.LSTM(self.mem_lstm_layer_sizes[h], self.mem_lstm_layer_sizes[h + 1], batch_first=True)]\n",
    "\n",
    "        #   After-LSTM\n",
    "        self.mem_after_lstm_layer_size = [self.mem_lstm_layer_sizes[-1]] + list(mem_after_lstm_hid_size)\n",
    "        for h in range(len(self.mem_after_lstm_layer_size)-1):\n",
    "            self.mem_after_lstm_layers += [nn.Linear(self.mem_after_lstm_layer_size[h],\n",
    "                                                     self.mem_after_lstm_layer_size[h+1]),\n",
    "                                           nn.ReLU()]\n",
    "\n",
    "        # Current Feature Extraction\n",
    "        cur_feature_layer_size = [obs_dim + act_dim] + list(cur_feature_hid_sizes)\n",
    "        for h in range(len(cur_feature_layer_size) - 1):\n",
    "            self.cur_feature_layers += [nn.Linear(cur_feature_layer_size[h], cur_feature_layer_size[h + 1]),\n",
    "                                        nn.ReLU()]\n",
    "\n",
    "        #    Memeory Gate\n",
    "        if self.mem_gate:\n",
    "            if self.mem_gate_before_current_feature_extraction:\n",
    "                # Put Memory Gate before feature extraction\n",
    "                self.mem_gate_layer += [\n",
    "                    nn.Linear(self.mem_after_lstm_layer_size[-1] + obs_dim + act_dim,\n",
    "                              self.mem_after_lstm_layer_size[-1]),\n",
    "                    nn.Sigmoid()]\n",
    "            else:\n",
    "                # Put Memory Gate after current feature extraction\n",
    "                self.mem_gate_layer += [\n",
    "                    nn.Linear(self.mem_after_lstm_layer_size[-1] + cur_feature_layer_size[-1],\n",
    "                              self.mem_after_lstm_layer_size[-1]),\n",
    "                    nn.Sigmoid()]\n",
    "\n",
    "        # Post-Combination\n",
    "        post_combined_layer_size = [self.mem_after_lstm_layer_size[-1] + cur_feature_layer_size[-1]] + list(\n",
    "            post_comb_hid_sizes) + [1]\n",
    "        for h in range(len(post_combined_layer_size) - 2):\n",
    "            self.post_combined_layers += [nn.Linear(post_combined_layer_size[h], post_combined_layer_size[h + 1]),\n",
    "                                          nn.ReLU()]\n",
    "        self.post_combined_layers += [nn.Linear(post_combined_layer_size[-2], post_combined_layer_size[-1]),\n",
    "                                      nn.Identity()]\n",
    "\n",
    "    def forward(self, obs, act, hist_obs, hist_act, hist_seg_len):\n",
    "        #\n",
    "        tmp_hist_seg_len = deepcopy(hist_seg_len)\n",
    "        tmp_hist_seg_len[hist_seg_len == 0] = 1\n",
    "        if self.hist_with_past_act:\n",
    "            x = torch.cat([hist_obs, hist_act], dim=-1)\n",
    "        else:\n",
    "            x = hist_obs\n",
    "        # Memory\n",
    "        #    Pre-LSTM\n",
    "        for layer in self.mem_pre_lstm_layers:\n",
    "            x = layer(x)\n",
    "        #    LSTM\n",
    "        for layer in self.mem_lstm_layers:\n",
    "            x, (lstm_hidden_state, lstm_cell_state) = layer(x)\n",
    "        #    After-LSTM\n",
    "        for layer in self.mem_after_lstm_layers:\n",
    "            x = layer(x)\n",
    "        #    History output mask to reduce disturbance cased by none history memory\n",
    "        hist_out = torch.gather(x, 1,\n",
    "                                (tmp_hist_seg_len - 1).view(-1, 1).repeat(1, self.mem_after_lstm_layer_size[-1]).unsqueeze(\n",
    "                                    1).long()).squeeze(1)\n",
    "        if self.use_hist_mask:\n",
    "            hist_msk = (hist_seg_len != 0).float().view(-1, 1).repeat(1, self.mem_after_lstm_layer_size[-1]).to(DEVICE)\n",
    "        else:\n",
    "            hist_msk = torch.ones(hist_out.size()).to(DEVICE)\n",
    "\n",
    "        # Current Feature Extraction\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        for layer in self.cur_feature_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Memory Gate\n",
    "        if self.mem_gate:\n",
    "            if self.mem_gate_before_current_feature_extraction:\n",
    "                # Put Memory Gate before feature extraction\n",
    "                memory_gate = torch.cat([hist_out * hist_msk, obs, act], dim=-1)\n",
    "                for layer in self.mem_gate_layer:\n",
    "                    memory_gate = layer(memory_gate)\n",
    "            else:\n",
    "                # Put Memory Gate after current feature extraction\n",
    "                memory_gate = torch.cat([hist_out * hist_msk, x], dim=-1)\n",
    "                for layer in self.mem_gate_layer:\n",
    "                    memory_gate = layer(memory_gate)\n",
    "        else:\n",
    "            memory_gate = torch.ones((1, 1)).to(DEVICE)  # Dummy value for logging memory_gate\n",
    "\n",
    "        # Post-Combination\n",
    "        if self.mem_gate:\n",
    "            extracted_memory = memory_gate * hist_out * hist_msk\n",
    "            x = torch.cat([extracted_memory, x], dim=-1)\n",
    "        else:\n",
    "            extracted_memory = hist_out * hist_msk\n",
    "            x = torch.cat([extracted_memory, x], dim=-1)\n",
    "\n",
    "        for layer in self.post_combined_layers:\n",
    "            x = layer(x)\n",
    "        # squeeze(x, -1) : critical to ensure q has right shape.\n",
    "        return torch.squeeze(x, -1), hist_out, memory_gate, extracted_memory\n",
    "\n",
    "\n",
    "class MLPActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit,\n",
    "                 mem_pre_lstm_hid_sizes=(128,),\n",
    "                 mem_lstm_hid_sizes=(128,),\n",
    "                 mem_after_lstm_hid_size=(128,),\n",
    "                 cur_feature_hid_sizes=(128,),\n",
    "                 post_comb_hid_sizes=(128,),\n",
    "                 mem_gate=True,\n",
    "                 mem_gate_before_current_feature_extraction=False,\n",
    "                 hist_with_past_act=False, use_hist_mask=False):\n",
    "        super(MLPActor, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.act_limit = act_limit\n",
    "        self.mem_gate = mem_gate\n",
    "        self.mem_gate_before_current_feature_extraction = mem_gate_before_current_feature_extraction\n",
    "        self.hist_with_past_act = hist_with_past_act\n",
    "        self.use_hist_mask = use_hist_mask\n",
    "        #\n",
    "        self.mem_pre_lstm_layers = nn.ModuleList()\n",
    "        self.mem_lstm_layers = nn.ModuleList()\n",
    "        self.mem_after_lstm_layers = nn.ModuleList()\n",
    "\n",
    "        self.mem_gate_layer = nn.ModuleList()\n",
    "\n",
    "        self.cur_feature_layers = nn.ModuleList()\n",
    "        self.post_combined_layers = nn.ModuleList()\n",
    "\n",
    "        # Memory\n",
    "        #    Pre-LSTM\n",
    "        if self.hist_with_past_act:\n",
    "            mem_pre_lstm_layer_size = [obs_dim + act_dim] + list(mem_pre_lstm_hid_sizes)\n",
    "        else:\n",
    "            mem_pre_lstm_layer_size = [obs_dim] + list(mem_pre_lstm_hid_sizes)\n",
    "        for h in range(len(mem_pre_lstm_layer_size) - 1):\n",
    "            self.mem_pre_lstm_layers += [nn.Linear(mem_pre_lstm_layer_size[h],\n",
    "                                                   mem_pre_lstm_layer_size[h + 1]),\n",
    "                                         nn.ReLU()]\n",
    "        #    LSTM\n",
    "        self.mem_lstm_layer_sizes = [mem_pre_lstm_layer_size[-1]] + list(mem_lstm_hid_sizes)\n",
    "        for h in range(len(self.mem_lstm_layer_sizes) - 1):\n",
    "            self.mem_lstm_layers += [\n",
    "                nn.LSTM(self.mem_lstm_layer_sizes[h], self.mem_lstm_layer_sizes[h + 1], batch_first=True)]\n",
    "        #   After-LSTM\n",
    "        self.mem_after_lstm_layer_size = [self.mem_lstm_layer_sizes[-1]] + list(mem_after_lstm_hid_size)\n",
    "        for h in range(len(self.mem_after_lstm_layer_size) - 1):\n",
    "            self.mem_after_lstm_layers += [nn.Linear(self.mem_after_lstm_layer_size[h],\n",
    "                                                     self.mem_after_lstm_layer_size[h + 1]),\n",
    "                                           nn.ReLU()]\n",
    "\n",
    "        #    Memeory Gate\n",
    "        if self.mem_gate:\n",
    "            if self.mem_gate_before_current_feature_extraction:\n",
    "                # Put Memory Gate before feature extraction\n",
    "                self.mem_gate_layer += [\n",
    "                    nn.Linear(self.mem_after_lstm_layer_size[-1] + obs_dim, self.mem_after_lstm_layer_size[-1]),\n",
    "                    nn.Sigmoid()]\n",
    "            else:\n",
    "                # Put Memory Gate after current feature extraction\n",
    "                self.mem_gate_layer += [\n",
    "                    nn.Linear(self.mem_after_lstm_layer_size[-1] + cur_feature_hid_sizes[-1],\n",
    "                              self.mem_after_lstm_layer_size[-1]),\n",
    "                    nn.Sigmoid()]\n",
    "\n",
    "        # Current Feature Extraction\n",
    "        cur_feature_layer_size = [obs_dim] + list(cur_feature_hid_sizes)\n",
    "        for h in range(len(cur_feature_layer_size) - 1):\n",
    "            self.cur_feature_layers += [nn.Linear(cur_feature_layer_size[h], cur_feature_layer_size[h + 1]),\n",
    "                                        nn.ReLU()]\n",
    "\n",
    "        # Post-Combination\n",
    "        post_combined_layer_size = [self.mem_after_lstm_layer_size[-1] + cur_feature_layer_size[-1]] + list(\n",
    "            post_comb_hid_sizes) + [act_dim]\n",
    "        for h in range(len(post_combined_layer_size) - 2):\n",
    "            self.post_combined_layers += [nn.Linear(post_combined_layer_size[h], post_combined_layer_size[h + 1]),\n",
    "                                          nn.ReLU()]\n",
    "        self.post_combined_layers += [nn.Linear(post_combined_layer_size[-2], post_combined_layer_size[-1]), nn.Tanh()]\n",
    "\n",
    "    def forward(self, obs, hist_obs, hist_act, hist_seg_len):\n",
    "        #\n",
    "        tmp_hist_seg_len = deepcopy(hist_seg_len)\n",
    "        tmp_hist_seg_len[hist_seg_len == 0] = 1\n",
    "        if self.hist_with_past_act:\n",
    "            x = torch.cat([hist_obs, hist_act], dim=-1)\n",
    "        else:\n",
    "            x = hist_obs\n",
    "        # Memory\n",
    "        #    Pre-LSTM\n",
    "        for layer in self.mem_pre_lstm_layers:\n",
    "            x = layer(x)\n",
    "        #    LSTM\n",
    "        for layer in self.mem_lstm_layers:\n",
    "            x, (lstm_hidden_state, lstm_cell_state) = layer(x)\n",
    "        #    After-LSTM\n",
    "        for layer in self.mem_after_lstm_layers:\n",
    "            x = layer(x)\n",
    "        hist_out = torch.gather(x, 1,\n",
    "                                (tmp_hist_seg_len - 1).view(-1, 1).repeat(1, self.mem_after_lstm_layer_size[-1]).unsqueeze(\n",
    "                                    1).long()).squeeze(1)\n",
    "        if self.use_hist_mask:\n",
    "            hist_msk = (hist_seg_len != 0).float().view(-1, 1).repeat(1, self.mem_after_lstm_layer_size[-1]).to(DEVICE)\n",
    "        else:\n",
    "            hist_msk = torch.ones(hist_out.size()).to(DEVICE)\n",
    "\n",
    "        # Current Feature Extraction\n",
    "        x = obs\n",
    "        for layer in self.cur_feature_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Memory Gate\n",
    "        if self.mem_gate:\n",
    "            if self.mem_gate_before_current_feature_extraction:\n",
    "                # Put Memory Gate before feature extraction\n",
    "                memory_gate = torch.cat([hist_out * hist_msk, obs], dim=-1)\n",
    "                for layer in self.mem_gate_layer:\n",
    "                    memory_gate = layer(memory_gate)\n",
    "            else:\n",
    "                # Put Memory Gate after current feature extraction\n",
    "                memory_gate = torch.cat([hist_out * hist_msk, x], dim=-1)\n",
    "                for layer in self.mem_gate_layer:\n",
    "                    memory_gate = layer(memory_gate)\n",
    "        else:\n",
    "            memory_gate = torch.ones((1, 1)).to(DEVICE)  # Dummy value for logging memory_gate\n",
    "\n",
    "        # Post-Combination\n",
    "        if self.mem_gate:\n",
    "            extracted_memory = memory_gate * hist_out * hist_msk\n",
    "            x = torch.cat([extracted_memory, x], dim=-1)\n",
    "        else:\n",
    "            extracted_memory = hist_out * hist_msk\n",
    "            x = torch.cat([extracted_memory, x], dim=-1)\n",
    "\n",
    "        for layer in self.post_combined_layers:\n",
    "            x = layer(x)\n",
    "        return self.act_limit * x, hist_out, memory_gate, extracted_memory\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit=1,\n",
    "                 critic_mem_pre_lstm_hid_sizes=(128,),\n",
    "                 critic_mem_lstm_hid_sizes=(128,),\n",
    "                 critic_mem_after_lstm_hid_size=(128,),\n",
    "                 critic_cur_feature_hid_sizes=(128,),\n",
    "                 critic_post_comb_hid_sizes=(128,),\n",
    "                 critic_mem_gate=True,\n",
    "                 critic_mem_gate_before_current_feature_extraction=False,\n",
    "                 critic_hist_with_past_act=False,\n",
    "                 critic_use_hist_mask=False,\n",
    "                 actor_mem_pre_lstm_hid_sizes=(128,),\n",
    "                 actor_mem_lstm_hid_sizes=(128,),\n",
    "                 actor_mem_after_lstm_hid_size=(128,),\n",
    "                 actor_cur_feature_hid_sizes=(128,),\n",
    "                 actor_post_comb_hid_sizes=(128,),\n",
    "                 actor_mem_gate=True,\n",
    "                 actor_mem_gate_before_current_feature_extraction=False,\n",
    "                 actor_hist_with_past_act=False,\n",
    "                 actor_use_hist_mask=False):\n",
    "        super(MLPActorCritic, self).__init__()\n",
    "        self.q1 = MLPCritic(obs_dim, act_dim,\n",
    "                            mem_pre_lstm_hid_sizes=critic_mem_pre_lstm_hid_sizes,\n",
    "                            mem_lstm_hid_sizes=critic_mem_lstm_hid_sizes,\n",
    "                            mem_after_lstm_hid_size=critic_mem_after_lstm_hid_size,\n",
    "                            cur_feature_hid_sizes=critic_cur_feature_hid_sizes,\n",
    "                            post_comb_hid_sizes=critic_post_comb_hid_sizes,\n",
    "                            mem_gate=critic_mem_gate,\n",
    "                            mem_gate_before_current_feature_extraction=critic_mem_gate_before_current_feature_extraction,\n",
    "                            hist_with_past_act=critic_hist_with_past_act,\n",
    "                            use_hist_mask=critic_use_hist_mask)\n",
    "        self.q2 = MLPCritic(obs_dim, act_dim,\n",
    "                            mem_pre_lstm_hid_sizes=critic_mem_pre_lstm_hid_sizes,\n",
    "                            mem_lstm_hid_sizes=critic_mem_lstm_hid_sizes,\n",
    "                            mem_after_lstm_hid_size=critic_mem_after_lstm_hid_size,\n",
    "                            cur_feature_hid_sizes=critic_cur_feature_hid_sizes,\n",
    "                            post_comb_hid_sizes=critic_post_comb_hid_sizes,\n",
    "                            mem_gate=critic_mem_gate,\n",
    "                            mem_gate_before_current_feature_extraction=critic_mem_gate_before_current_feature_extraction,\n",
    "                            hist_with_past_act=critic_hist_with_past_act,\n",
    "                            use_hist_mask=critic_use_hist_mask)\n",
    "        self.pi = MLPActor(obs_dim, act_dim, act_limit,\n",
    "                           mem_pre_lstm_hid_sizes=actor_mem_pre_lstm_hid_sizes,\n",
    "                           mem_lstm_hid_sizes=actor_mem_lstm_hid_sizes,\n",
    "                           mem_after_lstm_hid_size=actor_mem_after_lstm_hid_size,\n",
    "                           cur_feature_hid_sizes=actor_cur_feature_hid_sizes,\n",
    "                           post_comb_hid_sizes=actor_post_comb_hid_sizes,\n",
    "                           mem_gate=actor_mem_gate,\n",
    "                           mem_gate_before_current_feature_extraction=actor_mem_gate_before_current_feature_extraction,\n",
    "                           hist_with_past_act=actor_hist_with_past_act,\n",
    "                           use_hist_mask=actor_use_hist_mask)\n",
    "\n",
    "    def act(self, obs, hist_obs=None, hist_act=None, hist_seg_len=None):\n",
    "        if (hist_obs is None) or (hist_act is None) or (hist_seg_len is None):\n",
    "            hist_obs = torch.zeros(1, 1, self.obs_dim).to(DEVICE)\n",
    "            hist_act = torch.zeros(1, 1, self.act_dim).to(DEVICE)\n",
    "            hist_seg_len = torch.zeros(1).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            act, _, _, _ = self.pi(obs, hist_obs, hist_act, hist_seg_len)\n",
    "            return act.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "\n",
    "#######################################################################################\n",
    "def lstm_td3(env_name, seed=0,\n",
    "             steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99,\n",
    "             polyak=0.995, pi_lr=1e-3, q_lr=1e-3,\n",
    "             start_steps=10000,\n",
    "             update_after=1000, update_every=50, act_noise=0.1, target_noise=0.2,\n",
    "             noise_clip=0.5, policy_delay=2, num_test_episodes=10, max_ep_len=1000,\n",
    "             batch_size=100,\n",
    "             max_hist_len=100,\n",
    "             partially_observable=False,\n",
    "             pomdp_type = 'remove_velocity',\n",
    "             flicker_prob=0.2, random_noise_sigma=0.1, random_sensor_missing_prob=0.1,\n",
    "             critic_mem_pre_lstm_hid_sizes=(128,),\n",
    "             critic_mem_lstm_hid_sizes=(128,),\n",
    "             critic_mem_after_lstm_hid_size=(128,),\n",
    "             critic_cur_feature_hid_sizes=(128,),\n",
    "             critic_post_comb_hid_sizes=(128,),\n",
    "             critic_mem_gate=False,\n",
    "             critic_mem_gate_before_current_feature_extraction=False,\n",
    "             critic_hist_with_past_act=False,\n",
    "             critic_use_hist_mask=False,\n",
    "             actor_mem_pre_lstm_hid_sizes=(128,),\n",
    "             actor_mem_lstm_hid_sizes=(128,),\n",
    "             actor_mem_after_lstm_hid_size=(128,),\n",
    "             actor_cur_feature_hid_sizes=(128,),\n",
    "             actor_post_comb_hid_sizes=(128,),\n",
    "             actor_mem_gate=False,\n",
    "             actor_mem_gate_before_current_feature_extraction=False,\n",
    "             actor_hist_with_past_act=False,\n",
    "             actor_use_hist_mask=False,\n",
    "             logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "    Twin Delayed Deep Deterministic Policy Gradient (TD3)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "\n",
    "        actor_critic: The constructor method for a PyTorch Module with an ``act``\n",
    "            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.\n",
    "            The ``act`` method and ``pi`` module should accept batches of\n",
    "            observations as inputs, and ``q1`` and ``q2`` should accept a batch\n",
    "            of observations and a batch of actions as inputs. When called,\n",
    "            these should return:\n",
    "\n",
    "            ===========  ================  ======================================\n",
    "            Call         Output Shape      Description\n",
    "            ===========  ================  ======================================\n",
    "            ``act``      (batch, act_dim)  | Numpy array of actions for each\n",
    "                                           | observation.\n",
    "            ``pi``       (batch, act_dim)  | Tensor containing actions from policy\n",
    "                                           | given observations.\n",
    "            ``q1``       (batch,)          | Tensor containing one current estimate\n",
    "                                           | of Q* for the provided observations\n",
    "                                           | and actions. (Critical: make sure to\n",
    "                                           | flatten this!)\n",
    "            ``q2``       (batch,)          | Tensor containing the other current\n",
    "                                           | estimate of Q* for the provided observations\n",
    "                                           | and actions. (Critical: make sure to\n",
    "                                           | flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object\n",
    "            you provided to TD3.\n",
    "\n",
    "        seed (int): Seed for random number generators.\n",
    "\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs)\n",
    "            for the agent and the environment in each epoch.\n",
    "\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "\n",
    "        polyak (float): Interpolation factor in polyak averaging for target\n",
    "            networks. Target networks are updated towards main networks\n",
    "            according to:\n",
    "\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow\n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually\n",
    "            close to 1.)\n",
    "\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "\n",
    "        update_after (int): Number of env interactions to collect before\n",
    "            starting to do gradient descent updates. Ensures replay buffer\n",
    "            is full enough for useful updates.\n",
    "\n",
    "        update_every (int): Number of env interactions that should elapse\n",
    "            between gradient descent updates. Note: Regardless of how long\n",
    "            you wait between updates, the ratio of env steps to gradient steps\n",
    "            is locked to 1.\n",
    "\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to\n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "\n",
    "        target_noise (float): Stddev for smoothing noise added to target\n",
    "            policy.\n",
    "\n",
    "        noise_clip (float): Limit for absolute value of target policy\n",
    "            smoothing noise.\n",
    "\n",
    "        policy_delay (int): Policy will only be updated once every\n",
    "            policy_delay times for each update of the Q-networks.\n",
    "\n",
    "        num_test_episodes (int): Number of episodes to test the deterministic\n",
    "            policy at the end of each epoch.\n",
    "\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # device = torch.device(DEVICE)\n",
    "\n",
    "    # Wrapper environment if using POMDP\n",
    "    if partially_observable:\n",
    "        env = POMDPWrapper(env_name, pomdp_type, flicker_prob, random_noise_sigma, random_sensor_missing_prob)\n",
    "        test_env = POMDPWrapper(env_name, pomdp_type, flicker_prob, random_noise_sigma, random_sensor_missing_prob)\n",
    "    else:\n",
    "        env, test_env = gym.make(env_name), gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Create actor-critic module and target networks\n",
    "    ac = MLPActorCritic(obs_dim, act_dim, act_limit,\n",
    "                        critic_mem_pre_lstm_hid_sizes=critic_mem_pre_lstm_hid_sizes,\n",
    "                        critic_mem_lstm_hid_sizes=critic_mem_lstm_hid_sizes,\n",
    "                        critic_mem_after_lstm_hid_size=critic_mem_after_lstm_hid_size,\n",
    "                        critic_cur_feature_hid_sizes=critic_cur_feature_hid_sizes,\n",
    "                        critic_post_comb_hid_sizes=critic_post_comb_hid_sizes,\n",
    "                        critic_mem_gate=critic_mem_gate,\n",
    "                        critic_mem_gate_before_current_feature_extraction=critic_mem_gate_before_current_feature_extraction,\n",
    "                        critic_hist_with_past_act=critic_hist_with_past_act,\n",
    "                        critic_use_hist_mask=critic_use_hist_mask,\n",
    "                        actor_mem_pre_lstm_hid_sizes=actor_mem_pre_lstm_hid_sizes,\n",
    "                        actor_mem_lstm_hid_sizes=actor_mem_lstm_hid_sizes,\n",
    "                        actor_mem_after_lstm_hid_size=actor_mem_after_lstm_hid_size,\n",
    "                        actor_cur_feature_hid_sizes=actor_cur_feature_hid_sizes,\n",
    "                        actor_post_comb_hid_sizes=actor_post_comb_hid_sizes,\n",
    "                        actor_mem_gate=actor_mem_gate,\n",
    "                        actor_mem_gate_before_current_feature_extraction=actor_mem_gate_before_current_feature_extraction,\n",
    "                        actor_hist_with_past_act=actor_hist_with_past_act,\n",
    "                        actor_use_hist_mask=actor_use_hist_mask)\n",
    "    ac_targ = deepcopy(ac)\n",
    "    ac.to(DEVICE)\n",
    "    ac_targ.to(DEVICE)\n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # List of parameters for both Q-networks (save this for convenience)\n",
    "    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, max_size=replay_size)\n",
    "\n",
    "    # # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "    # var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q1, ac.q2])\n",
    "    # logger.log('\\nNumber of parameters: \\t pi: %d, \\t q1: %d, \\t q2: %d\\n' % var_counts)\n",
    "\n",
    "    # Set up function for computing TD3 Q-losses\n",
    "    def compute_loss_q(data):\n",
    "        o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "        h_o, h_a, h_o2, h_a2, h_len = data['hist_obs'], data['hist_act'], data['hist_obs2'], data['hist_act2'], data[\n",
    "            'hist_len']\n",
    "\n",
    "        q1, q1_hist_out, q1_memory_gate, q1_extracted_memory = ac.q1(o, a, h_o, h_a, h_len)\n",
    "        q2, q2_hist_out, q2_memory_gate, q2_extracted_memory = ac.q2(o, a, h_o, h_a, h_len)\n",
    "\n",
    "        # Bellman backup for Q functions\n",
    "        with torch.no_grad():\n",
    "            pi_targ, _, _, _ = ac_targ.pi(o2, h_o2, h_a2, h_len)\n",
    "\n",
    "            # Target policy smoothing\n",
    "            epsilon = torch.randn_like(pi_targ) * target_noise\n",
    "            epsilon = torch.clamp(epsilon, -noise_clip, noise_clip)\n",
    "            a2 = pi_targ + epsilon\n",
    "            a2 = torch.clamp(a2, -act_limit, act_limit)\n",
    "\n",
    "            # Target Q-values\n",
    "            q1_pi_targ, _, _, _ = ac_targ.q1(o2, a2, h_o2, h_a2, h_len)\n",
    "            q2_pi_targ, _, _, _ = ac_targ.q2(o2, a2, h_o2, h_a2, h_len)\n",
    "            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "            backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "        # MSE loss against Bellman backup\n",
    "        loss_q1 = ((q1 - backup) ** 2).mean()\n",
    "        loss_q2 = ((q2 - backup) ** 2).mean()\n",
    "        loss_q = loss_q1 + loss_q2\n",
    "\n",
    "        # Useful info for logging\n",
    "        # import pdb; pdb.set_trace()\n",
    "        loss_info = dict(Q1Vals=q1.detach().cpu().numpy(),\n",
    "                         Q2Vals=q2.detach().cpu().numpy(),\n",
    "                         Q1HistOut=q1_hist_out.mean(dim=1).detach().cpu().numpy(),\n",
    "                         Q2Histout=q2_hist_out.mean(dim=1).detach().cpu().numpy(),\n",
    "                         Q1MemoryGate=q1_memory_gate.mean(dim=1).detach().cpu().numpy(),\n",
    "                         Q2MemoryGate=q2_memory_gate.mean(dim=1).detach().cpu().numpy(),\n",
    "                         Q1ExtractedMemory=q1_extracted_memory.mean(dim=1).detach().cpu().numpy(),\n",
    "                         Q2ExtractedMemory=q2_extracted_memory.mean(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        return loss_q, loss_info\n",
    "\n",
    "    # Set up function for computing TD3 pi loss\n",
    "    def compute_loss_pi(data):\n",
    "        o, h_o, h_a, h_len = data['obs'], data['hist_obs'], data['hist_act'], data['hist_len']\n",
    "        a, a_hist_out, a_memory_gate, a_extracted_memory = ac.pi(o, h_o, h_a, h_len)\n",
    "        q1_pi, _, _, _ = ac.q1(o, a, h_o, h_a, h_len)\n",
    "        loss_info = dict(ActHistOut=a_hist_out.mean(dim=1).detach().cpu().numpy(),\n",
    "                         ActMemoryGate=a_memory_gate.mean(dim=1).detach().cpu().numpy(),\n",
    "                         ActExtractedMemory=a_extracted_memory.mean(dim=1).detach().cpu().numpy())\n",
    "        return -q1_pi.mean(), loss_info\n",
    "\n",
    "    # Set up optimizers for policy and q-function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    q_optimizer = Adam(q_params, lr=q_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "    logger.setup_pytorch_saver(ac)\n",
    "    \n",
    "    def update(data, timer):\n",
    "        # First run one gradient descent step for Q1 and Q2\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q, loss_info = compute_loss_q(data)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # Record things\n",
    "        logger.store(LossQ=loss_q.item(), **loss_info)\n",
    "\n",
    "        # Possibly update pi and target networks\n",
    "        if timer % policy_delay == 0:\n",
    "            # Freeze Q-networks so you don't waste computational effort\n",
    "            # computing gradients for them during the policy learning step.\n",
    "            for p in q_params:\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # Next run one gradient descent step for pi.\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, loss_info_pi = compute_loss_pi(data)\n",
    "            loss_pi.backward()\n",
    "            pi_optimizer.step()\n",
    "\n",
    "            # Unfreeze Q-networks so you can optimize it at next DDPG step.\n",
    "            for p in q_params:\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # Record things\n",
    "            logger.store(LossPi=loss_pi.item(), **loss_info_pi)\n",
    "\n",
    "            # Finally, update target networks by polyak averaging.\n",
    "            with torch.no_grad():\n",
    "                for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                    # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                    # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                    p_targ.data.mul_(polyak)\n",
    "                    p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "    def get_action(o, o_buff, a_buff, o_buff_len, noise_scale):\n",
    "        h_o = torch.tensor(o_buff).view(1, o_buff.shape[0], o_buff.shape[1]).float().to(DEVICE)\n",
    "        h_a = torch.tensor(a_buff).view(1, a_buff.shape[0], a_buff.shape[1]).float().to(DEVICE)\n",
    "        h_l = torch.tensor([o_buff_len]).float().to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            a = ac.act(torch.as_tensor(o, dtype=torch.float32).view(1, -1).to(DEVICE),\n",
    "                       h_o, h_a, h_l).reshape(act_dim)\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent():\n",
    "        for j in range(num_test_episodes):\n",
    "            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "\n",
    "            if max_hist_len > 0:\n",
    "                o_buff = np.zeros([max_hist_len, obs_dim])\n",
    "                a_buff = np.zeros([max_hist_len, act_dim])\n",
    "                o_buff[0, :] = o\n",
    "                o_buff_len = 0\n",
    "            else:\n",
    "                o_buff = np.zeros([1, obs_dim])\n",
    "                a_buff = np.zeros([1, act_dim])\n",
    "                o_buff_len = 0\n",
    "\n",
    "            while not (d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                a = get_action(o, o_buff, a_buff, o_buff_len, 0)\n",
    "                o2, r, d, _ = test_env.step(a)\n",
    "\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "                # Add short history\n",
    "                if max_hist_len != 0:\n",
    "                    if o_buff_len == max_hist_len:\n",
    "                        o_buff[:max_hist_len - 1] = o_buff[1:]\n",
    "                        a_buff[:max_hist_len - 1] = a_buff[1:]\n",
    "                        o_buff[max_hist_len - 1] = list(o)\n",
    "                        a_buff[max_hist_len - 1] = list(a)\n",
    "                    else:\n",
    "                        o_buff[o_buff_len + 1 - 1] = list(o)\n",
    "                        a_buff[o_buff_len + 1 - 1] = list(a)\n",
    "                        o_buff_len += 1\n",
    "                o = o2\n",
    "\n",
    "            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    if max_hist_len > 0:\n",
    "        o_buff = np.zeros([max_hist_len, obs_dim])\n",
    "        a_buff = np.zeros([max_hist_len, act_dim])\n",
    "        o_buff[0, :] = o\n",
    "        o_buff_len = 0\n",
    "    else:\n",
    "        o_buff = np.zeros([1, obs_dim])\n",
    "        a_buff = np.zeros([1, act_dim])\n",
    "        o_buff_len = 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    start_time = time.time()\n",
    "    for t in range(total_steps):\n",
    "        # if t % 200 == 0:\n",
    "        #     end_time = time.time()\n",
    "        #     print(\"t={}, {}s\".format(t, end_time - start_time))\n",
    "        #     start_time = end_time\n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards,\n",
    "        # use the learned policy (with some noise, via act_noise).\n",
    "        if t > start_steps:\n",
    "            a = get_action(o, o_buff, a_buff, o_buff_len, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len == max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Add short history\n",
    "        if max_hist_len != 0:\n",
    "            if o_buff_len == max_hist_len:\n",
    "                o_buff[:max_hist_len - 1] = o_buff[1:]\n",
    "                a_buff[:max_hist_len - 1] = a_buff[1:]\n",
    "                o_buff[max_hist_len - 1] = list(o)\n",
    "                a_buff[max_hist_len - 1] = list(a)\n",
    "            else:\n",
    "                o_buff[o_buff_len + 1 - 1] = list(o)\n",
    "                a_buff[o_buff_len + 1 - 1] = list(a)\n",
    "                o_buff_len += 1\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update\n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "\n",
    "        # End of trajectory handling\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "            if max_hist_len > 0:\n",
    "                o_buff = np.zeros([max_hist_len, obs_dim])\n",
    "                a_buff = np.zeros([max_hist_len, act_dim])\n",
    "                o_buff[0, :] = o\n",
    "                o_buff_len = 0\n",
    "            else:\n",
    "                o_buff = np.zeros([1, obs_dim])\n",
    "                a_buff = np.zeros([1, act_dim])\n",
    "                o_buff_len = 0\n",
    "\n",
    "        # Update handling\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for j in range(update_every):\n",
    "                batch = replay_buffer.sample_batch_with_history(batch_size, max_hist_len)\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "                update(data=batch, timer=j)\n",
    "\n",
    "        # End of epoch handling\n",
    "        if (t + 1) % steps_per_epoch == 0:\n",
    "            epoch = (t + 1) // steps_per_epoch\n",
    "\n",
    "            # Save model\n",
    "            if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "                # logger.save_state({'env': env}, None)\n",
    "                # Save all necessaries to resume the learning:\n",
    "                #    1. all hyper-parameters\n",
    "                #    2. env\n",
    "                #    2. replay_buffer\n",
    "                #    3. current step\n",
    "                #    4. current observation_buffer, action_buffer, observation_buffer_length, observation\n",
    "                #    5. online Actor-Critic\n",
    "                #    6. target Actor-Critic\n",
    "                #    7. optimizers\n",
    "                \n",
    "                fpath = 'pyt_save'\n",
    "                fpath = osp.join(logger.output_dir, fpath)\n",
    "                fname = 'checkpoint-' + ('%d'%epoch if epoch is not None else '') + '.pt'\n",
    "                fname = osp.join(fpath, fname)\n",
    "                os.makedirs(fpath, exist_ok=True)\n",
    "                saver_elements = {'env': env, 'replay_buffer': replay_buffer, \n",
    "                                  'start_time': start_time,\n",
    "                                  'o': o, 'ep_ret': ep_ret, 'ep_len': ep_len, 't': t, \n",
    "                                  'o_buff': o_buff, 'a_buff': a_buff, 'o_buff_len': o_buff_len,\n",
    "                                  'ac_state_dict': ac.state_dict(),\n",
    "                                  'target_ac_state_dict': ac_targ.state_dict(),\n",
    "                                  'pi_optimizer_state_dict': pi_optimizer,\n",
    "                                  'q_optimizer_state_dict': q_optimizer}\n",
    "                torch.save(saver_elements, fname)\n",
    "                import pdb; pdb.set_trace()\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            test_agent()\n",
    "\n",
    "            # Log info about epoch\n",
    "            logger.log_tabular('Epoch', epoch)\n",
    "            logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('EpLen', average_only=True)\n",
    "            logger.log_tabular('TestEpLen', average_only=True)\n",
    "            logger.log_tabular('TotalEnvInteracts', t)\n",
    "            logger.log_tabular('Q1Vals', with_min_and_max=True)\n",
    "            logger.log_tabular('Q2Vals', with_min_and_max=True)\n",
    "            logger.log_tabular('Q1HistOut', with_min_and_max=True)\n",
    "            logger.log_tabular('Q2Histout', with_min_and_max=True)\n",
    "            logger.log_tabular('Q1MemoryGate', with_min_and_max=True)\n",
    "            logger.log_tabular('Q2MemoryGate', with_min_and_max=True)\n",
    "            logger.log_tabular('Q1ExtractedMemory', with_min_and_max=True)\n",
    "            logger.log_tabular('Q2ExtractedMemory', with_min_and_max=True)\n",
    "            logger.log_tabular('ActHistOut', with_min_and_max=True)\n",
    "            logger.log_tabular('ActMemoryGate', with_min_and_max=True)\n",
    "            logger.log_tabular('ActExtractedMemory', with_min_and_max=True)\n",
    "            logger.log_tabular('LossPi', average_only=True)\n",
    "            logger.log_tabular('LossQ', average_only=True)\n",
    "\n",
    "            logger.log_tabular('Time', time.time() - start_time)\n",
    "            logger.dump_tabular()\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"Function used in argument parser for converting string to bool.\"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def list2tuple(v):\n",
    "    return tuple(v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'pyt_save'\n",
    "fpath = osp.join(logger.output_dir, fpath)\n",
    "fname = 'checkpoint-' + ('%d'%epoch if epoch is not None else '') + '.pt'\n",
    "fname = osp.join(fpath, fname)\n",
    "os.makedirs(fpath, exist_ok=True)\n",
    "torch.save({'env': env, 'replay_buffer': replay_buffer, \n",
    "            'start_time': start_time,\n",
    "            'o': o, 'ep_ret': ep_ret, 'ep_len': ep_len, 't': t,\n",
    "            'o_buff': o_buff, 'a_buff': a_buff, 'o_buff_len': o_buff_len,\n",
    "            'ac_state_dict': ac.state_dict(),\n",
    "            'target_ac_state_dict': ac_targ.state_dict(),\n",
    "            'pi_optimizer_state_dict': pi_optimizer,\n",
    "            'q_optimizer_state_dict': q_optimizer\n",
    "           }, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "#     parser.add_argument('--epochs', type=int, default=400)\n",
    "#     parser.add_argument('--max_hist_len', type=int, default=5)\n",
    "#     parser.add_argument('--partially_observable', type=str2bool, nargs='?', const=True, default=False, help=\"Using POMDP\")\n",
    "#     parser.add_argument('--pomdp_type',\n",
    "#                         choices=['remove_velocity', 'flickering', 'random_noise', 'random_sensor_missing',\n",
    "#                                  'remove_velocity_and_flickering', 'remove_velocity_and_random_noise',\n",
    "#                                  'remove_velocity_and_random_sensor_missing', 'flickering_and_random_noise',\n",
    "#                                  'random_noise_and_random_sensor_missing', 'random_sensor_missing_and_random_noise'],\n",
    "#                         default='remove_velocity')\n",
    "#     parser.add_argument('--flicker_prob', type=float, default=0.2)\n",
    "#     parser.add_argument('--random_noise_sigma', type=float, default=0.1)\n",
    "#     parser.add_argument('--random_sensor_missing_prob', type=float, default=0.1)\n",
    "#     parser.add_argument('--critic_mem_pre_lstm_hid_sizes', type=int, nargs=\"+\", default=[128])\n",
    "#     parser.add_argument('--critic_mem_lstm_hid_sizes', type=int, nargs=\"+\", default=[128])\n",
    "#     parser.add_argument('--critic_mem_after_lstm_hid_size', type=int, nargs=\"+\", default=[])\n",
    "#     parser.add_argument('--critic_cur_feature_hid_sizes', type=int, nargs=\"+\", default=[128, 128])\n",
    "#     parser.add_argument('--critic_post_comb_hid_sizes', type=int, nargs=\"+\", default=[128])\n",
    "#     parser.add_argument('--critic_mem_gate', type=str2bool, nargs='?', const=True, default=True)\n",
    "#     parser.add_argument('--critic_mem_gate_before_current_feature_extraction', type=str2bool, nargs='?',\n",
    "#                         const=True, default=True)\n",
    "#     parser.add_argument('--critic_hist_with_past_act', type=str2bool, nargs='?', const=True, default=True)\n",
    "#     parser.add_argument('--critic_use_hist_mask', type=str2bool, nargs='?', const=True, default=False)\n",
    "#     parser.add_argument('--actor_mem_pre_lstm_hid_sizes', type=int, nargs=\"+\", default=[128])\n",
    "#     parser.add_argument('--actor_mem_lstm_hid_sizes', type=int, nargs=\"+\", default=[128])\n",
    "#     parser.add_argument('--actor_mem_after_lstm_hid_size', type=int, nargs=\"+\", default=[])\n",
    "#     parser.add_argument('--actor_cur_feature_hid_sizes', type=int, nargs=\"+\", default=[128, 128])\n",
    "#     parser.add_argument('--actor_post_comb_hid_sizes', type=int, nargs=\"+\", default=[128])\n",
    "#     parser.add_argument('--actor_mem_gate', type=str2bool, nargs='?', const=True, default=True)\n",
    "#     parser.add_argument('--actor_mem_gate_before_current_feature_extraction', type=str2bool, nargs='?',\n",
    "#                         const=True, default=True)\n",
    "#     parser.add_argument('--actor_hist_with_past_act', type=str2bool, nargs='?', const=True, default=True)\n",
    "#     parser.add_argument('--actor_use_hist_mask', type=str2bool, nargs='?', const=True, default=False)\n",
    "#     parser.add_argument('--exp_name', type=str, default='lstm_td3')\n",
    "#     parser.add_argument(\"--data_dir\", type=str, default='spinup_data_lstm_gate')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.env = 'HalfCheetah-v2'\n",
    "        self.gamma = 0.99\n",
    "        self.seed = 0\n",
    "        self.epochs = 400\n",
    "        self.max_hist_len = 5\n",
    "        self.partially_observable = False \n",
    "#         choices=['remove_velocity', 'flickering', 'random_noise', 'random_sensor_missing',\n",
    "#                                      'remove_velocity_and_flickering', 'remove_velocity_and_random_noise',\n",
    "#                                      'remove_velocity_and_random_sensor_missing', 'flickering_and_random_noise',\n",
    "#                                      'random_noise_and_random_sensor_missing', 'random_sensor_missing_and_random_noise']\n",
    "        self.pomdp_type = 'remove_velocity'\n",
    "        self.flicker_prob = 0.2\n",
    "        self.random_noise_sigma = 0.1\n",
    "        self.random_sensor_missing_prob = 0.1\n",
    "        self.critic_mem_pre_lstm_hid_sizes = [128] \n",
    "        self.critic_mem_lstm_hid_sizes = [128] \n",
    "        self.critic_mem_after_lstm_hid_size = [] \n",
    "        self.critic_cur_feature_hid_sizes = [128, 128] \n",
    "        self.critic_post_comb_hid_sizes = [128] \n",
    "        self.critic_mem_gate = True \n",
    "        self.critic_mem_gate_before_current_feature_extraction = True \n",
    "        self.critic_hist_with_past_act = True \n",
    "        self.critic_use_hist_mask = False \n",
    "        self.actor_mem_pre_lstm_hid_sizes = [128] \n",
    "        self.actor_mem_lstm_hid_sizes = [128] \n",
    "        self.actor_mem_after_lstm_hid_size = [] \n",
    "        self.actor_cur_feature_hid_sizes = [128, 128] \n",
    "        self.actor_post_comb_hid_sizes = [128] \n",
    "        self.actor_mem_gate = True \n",
    "        self.actor_mem_gate_before_current_feature_extraction = True \n",
    "        self.actor_hist_with_past_act = True \n",
    "        self.actor_use_hist_mask = False \n",
    "        self.exp_name = 'lstm_td3' \n",
    "        self.data_dir = 'spinup_data_lstm_gate' \n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinningup_new\\\\spinup\\\\algos\\\\pytorch\\\\lstm_td3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_folder = globals()['_dh'][0]\n",
    "current_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to C:\\Users\\Lingheng\\Google Drive\\git_repos_2020\\spinup_data_lstm_gate\\2021-01-18_lstm_td3\\2021-01-18_11-48-18-lstm_td3_s0\\progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"act_noise\":\t0.1,\n",
      "    \"actor_cur_feature_hid_sizes\":\t[\n",
      "        128,\n",
      "        128\n",
      "    ],\n",
      "    \"actor_hist_with_past_act\":\ttrue,\n",
      "    \"actor_mem_after_lstm_hid_size\":\t[],\n",
      "    \"actor_mem_gate\":\ttrue,\n",
      "    \"actor_mem_gate_before_current_feature_extraction\":\ttrue,\n",
      "    \"actor_mem_lstm_hid_sizes\":\t[\n",
      "        128\n",
      "    ],\n",
      "    \"actor_mem_pre_lstm_hid_sizes\":\t[\n",
      "        128\n",
      "    ],\n",
      "    \"actor_post_comb_hid_sizes\":\t[\n",
      "        128\n",
      "    ],\n",
      "    \"actor_use_hist_mask\":\tfalse,\n",
      "    \"batch_size\":\t100,\n",
      "    \"critic_cur_feature_hid_sizes\":\t[\n",
      "        128,\n",
      "        128\n",
      "    ],\n",
      "    \"critic_hist_with_past_act\":\ttrue,\n",
      "    \"critic_mem_after_lstm_hid_size\":\t[],\n",
      "    \"critic_mem_gate\":\ttrue,\n",
      "    \"critic_mem_gate_before_current_feature_extraction\":\ttrue,\n",
      "    \"critic_mem_lstm_hid_sizes\":\t[\n",
      "        128\n",
      "    ],\n",
      "    \"critic_mem_pre_lstm_hid_sizes\":\t[\n",
      "        128\n",
      "    ],\n",
      "    \"critic_post_comb_hid_sizes\":\t[\n",
      "        128\n",
      "    ],\n",
      "    \"critic_use_hist_mask\":\tfalse,\n",
      "    \"env_name\":\t\"HalfCheetah-v2\",\n",
      "    \"epochs\":\t400,\n",
      "    \"exp_name\":\t\"lstm_td3\",\n",
      "    \"flicker_prob\":\t0.2,\n",
      "    \"gamma\":\t0.99,\n",
      "    \"logger\":\t{\n",
      "        \"<spinup.utils.logx.EpochLogger object at 0x00000174EE343188>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\t\"lstm_td3\",\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinup_data_lstm_gate\\\\2021-01-18_lstm_td3\\\\2021-01-18_11-48-18-lstm_td3_s0\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='C:\\\\\\\\Users\\\\\\\\Lingheng\\\\\\\\Google Drive\\\\\\\\git_repos_2020\\\\\\\\spinup_data_lstm_gate\\\\\\\\2021-01-18_lstm_td3\\\\\\\\2021-01-18_11-48-18-lstm_td3_s0\\\\\\\\progress.txt' mode='w' encoding='cp1252'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{\n",
      "        \"exp_name\":\t\"lstm_td3\",\n",
      "        \"output_dir\":\t\"C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinup_data_lstm_gate\\\\2021-01-18_lstm_td3\\\\2021-01-18_11-48-18-lstm_td3_s0\"\n",
      "    },\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"max_hist_len\":\t5,\n",
      "    \"noise_clip\":\t0.5,\n",
      "    \"num_test_episodes\":\t10,\n",
      "    \"partially_observable\":\tfalse,\n",
      "    \"pi_lr\":\t0.001,\n",
      "    \"policy_delay\":\t2,\n",
      "    \"polyak\":\t0.995,\n",
      "    \"pomdp_type\":\t\"remove_velocity\",\n",
      "    \"q_lr\":\t0.001,\n",
      "    \"random_noise_sigma\":\t0.1,\n",
      "    \"random_sensor_missing_prob\":\t0.1,\n",
      "    \"replay_size\":\t1000000,\n",
      "    \"save_freq\":\t1,\n",
      "    \"seed\":\t0,\n",
      "    \"start_steps\":\t10000,\n",
      "    \"steps_per_epoch\":\t4000,\n",
      "    \"target_noise\":\t0.2,\n",
      "    \"update_after\":\t1000,\n",
      "    \"update_every\":\t50\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-55-5ad602369e47>\u001b[0m(454)\u001b[0;36mlstm_td3\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    452 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    453 \u001b[1;33m            \u001b[1;31m# Test the performance of the deterministic version of the agent.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 454 \u001b[1;33m            \u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    455 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    456 \u001b[1;33m            \u001b[1;31m# Log info about epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> logger.output_dir\n",
      "'C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinup_data_lstm_gate\\\\2021-01-18_lstm_td3\\\\2021-01-18_11-48-18-lstm_td3_s0'\n",
      "ipdb> fpath = 'pyt_save' fpath = osp.join(logger.output_dir, fpath) fname = 'checkpoint-' + ('%d'%epoch if epoch is not None else '') + '.pt' fname = osp.join(fpath, fname) os.makedirs(fpath, exist_ok=True)\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> fname\n",
      "'C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinup_data_lstm_gate\\\\2021-01-18_lstm_td3\\\\2021-01-18_11-48-18-lstm_td3_s0\\\\pyt_save\\\\model1.pt'\n"
     ]
    }
   ],
   "source": [
    "# Set log data saving directory\n",
    "from spinup.utils.run_utils import setup_logger_kwargs\n",
    "data_dir = osp.join(\n",
    "    osp.dirname(osp.dirname(osp.dirname(osp.dirname(osp.dirname(current_folder))))),\n",
    "    args.data_dir)\n",
    "logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed, data_dir, datestamp=True)\n",
    "\n",
    "lstm_td3(args.env,\n",
    "         gamma=args.gamma, seed=args.seed, epochs=args.epochs,\n",
    "         max_hist_len=args.max_hist_len,\n",
    "         partially_observable=args.partially_observable,\n",
    "         pomdp_type=args.pomdp_type,\n",
    "         flicker_prob=args.flicker_prob,\n",
    "         random_noise_sigma=args.random_noise_sigma,\n",
    "         random_sensor_missing_prob=args.random_sensor_missing_prob,\n",
    "         critic_mem_pre_lstm_hid_sizes=tuple(args.critic_mem_pre_lstm_hid_sizes),\n",
    "         critic_mem_lstm_hid_sizes=tuple(args.critic_mem_lstm_hid_sizes),\n",
    "         critic_mem_after_lstm_hid_size=tuple(args.critic_mem_after_lstm_hid_size),\n",
    "         critic_cur_feature_hid_sizes=tuple(args.critic_cur_feature_hid_sizes),\n",
    "         critic_post_comb_hid_sizes=tuple(args.critic_post_comb_hid_sizes),\n",
    "         critic_mem_gate=args.critic_mem_gate,\n",
    "         critic_mem_gate_before_current_feature_extraction=args.critic_mem_gate_before_current_feature_extraction,\n",
    "         critic_hist_with_past_act=args.critic_hist_with_past_act,\n",
    "         actor_mem_pre_lstm_hid_sizes=tuple(args.actor_mem_pre_lstm_hid_sizes),\n",
    "         actor_mem_lstm_hid_sizes=tuple(args.actor_mem_lstm_hid_sizes),\n",
    "         actor_mem_after_lstm_hid_size=tuple(args.actor_mem_after_lstm_hid_size),\n",
    "         actor_cur_feature_hid_sizes=tuple(args.actor_cur_feature_hid_sizes),\n",
    "         actor_post_comb_hid_sizes=tuple(args.actor_post_comb_hid_sizes),\n",
    "         actor_mem_gate=args.actor_mem_gate,\n",
    "         actor_mem_gate_before_current_feature_extraction=args.actor_mem_gate_before_current_feature_extraction,\n",
    "         actor_hist_with_past_act=args.actor_hist_with_past_act,\n",
    "         logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r\"C:\\Users\\Lingheng\\Google Drive\\git_repos_2020\\spinup_data_lstm_gate\\2021-01-17_lstm_td3\\2021-01-17_22-34-35-lstm_td3_s0\\vars.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'rb') as fo:\n",
    "    restored_vars = joblib.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': <TimeLimit<HalfCheetahEnv instance>>,\n",
       " 'replay_buffer': <__main__.ReplayBuffer at 0x174cf7825c8>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(17,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_vars['env'].action_space\n",
    "restored_vars['env'].observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_vars['replay_buffer'].ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_vars['replay_buffer'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_vars['replay_buffer'].max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = r\"C:\\Users\\Lingheng\\Google Drive\\git_repos_2020\\spinup_data_lstm_gate\\2021-01-17_lstm_td3\\2021-01-17_22-34-35-lstm_td3_s0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osp.join(dirname, 'compressed_vars.z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(dirname, 'compressed_vars.z'), 'wb') as fo:\n",
    "    joblib.dump(restored_vars['replay_buffer'], fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_vars['env'].action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(obs_dim=restored_vars['env'].action_space.shape[0], \n",
    "                             act_dim=restored_vars['env'].observation_space.shape[0], \n",
    "                             max_size=int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(dirname, 'compressed_replay_buffer.pkl'), 'wb') as fo:\n",
    "    joblib.dump([replay_buffer, test_var], fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(osp.join(dirname, 'compressed_replay_buffer_torch.pkl'), 'wb') as fo:\n",
    "    torch.save({'replay_buffer': replay_buffer}, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r\"C:\\Users\\Lingheng\\Google Drive\\git_repos_2020\\spinup_data_lstm_gate\\2021-01-17_lstm_td3\\2021-01-17_22-34-35-lstm_td3_s0\\compressed_replay_buffer_torch.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'rb') as fo:\n",
    "    torch_restored_vars = torch.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_restored_vars['replay_buffer'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
