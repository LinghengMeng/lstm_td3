{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import pybulletgym\n",
    "import time\n",
    "import spinup.algos.pytorch.td3_ow.core as core\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.env_wrapper.pomdp_wrapper import POMDPWrapper\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for TD3 agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n",
    "    \n",
    "    def sample_batch_ow(self, batch_size=32, observation_window_size=5, add_past_action=False):\n",
    "        \"\"\"\n",
    "        Sample observation within a window\n",
    "        \"\"\"\n",
    "        idxs = np.random.randint(observation_window_size-1, self.size, size=batch_size)\n",
    " \n",
    "        ow_obs = np.zeros([batch_size, observation_window_size, self.obs_dim])\n",
    "        ow_obs2 = np.zeros([batch_size, observation_window_size, self.obs_dim])\n",
    "        ow_act = np.zeros([batch_size, observation_window_size-1, self.act_dim])\n",
    "        ow_act2 = np.zeros([batch_size, observation_window_size-1, self.act_dim])\n",
    "        ow_rew = np.zeros([batch_size, observation_window_size])\n",
    "        ow_done = np.zeros([batch_size, observation_window_size])\n",
    "        for i in range(observation_window_size):\n",
    "            ow_obs[:, -1-i, :] = self.obs_buf[idxs-i, :]\n",
    "            ow_obs2[:, -1-i, :] = self.obs2_buf[idxs-i, :]\n",
    "            if i < (observation_window_size-1):    # Only add actions before the current observation\n",
    "                ow_act[:, -1-i, :] = self.act_buf[idxs-i-1, :]\n",
    "                ow_act2[:, -1-i, :] = self.act_buf[idxs-i, :]\n",
    "            ow_done[:, -1-i] = self.done_buf[idxs-i]\n",
    "        # If there is a done in the observation window that is not the last one,\n",
    "        # then set all observations before that to 0.\n",
    "        x_idxs, y_idxs = np.where(ow_done[:, :-1]==1)\n",
    "        for i, x in enumerate(x_idxs):\n",
    "            y = y_idxs[i]\n",
    "            for pre_y in range(0, y+1):\n",
    "                ow_obs[x, pre_y] = np.zeros([self.obs_dim])\n",
    "                ow_obs2[x, pre_y] = np.zeros([self.obs_dim])\n",
    "                ow_act[x, pre_y] = np.zeros([self.act_dim])\n",
    "                ow_act2[x, pre_y] = np.zeros([self.act_dim])\n",
    "\n",
    "        # Construct batch data\n",
    "        if add_past_action:\n",
    "            # Combine past action and observation within the window\n",
    "            comb_past_act_obs = np.zeros([batch_size, int((self.obs_dim+self.act_dim)*(observation_window_size-1)+self.obs_dim)])\n",
    "            comb_past_act_obs2 = np.zeros([batch_size, int((self.obs_dim+self.act_dim)*(observation_window_size-1)+self.obs_dim)])\n",
    "            for i in range(observation_window_size):\n",
    "                if i < (observation_window_size-1):\n",
    "                    comb_past_act_obs[:, i*(self.obs_dim+self.act_dim):(i+1)*(self.obs_dim+self.act_dim)] = np.concatenate((ow_obs[:, i], ow_act[:, i]), axis=1)\n",
    "                    comb_past_act_obs2[:, i*(self.obs_dim+self.act_dim):(i+1)*(self.obs_dim+self.act_dim)] = np.concatenate((ow_obs2[:, i], ow_act2[:, i]), axis=1)\n",
    "                else:\n",
    "                    comb_past_act_obs[:, -self.obs_dim:] = ow_obs[:, -1]\n",
    "                    comb_past_act_obs2[:, -self.obs_dim:] = ow_obs2[:, -1]\n",
    "            \n",
    "            batch = dict(obs=comb_past_act_obs,\n",
    "                         obs2=comb_past_act_obs2,\n",
    "                         act=self.act_buf[idxs],\n",
    "                         rew=self.rew_buf[idxs],\n",
    "                         done=self.done_buf[idxs])\n",
    "        else:\n",
    "            batch = dict(obs=ow_obs.reshape([batch_size, -1]),    # concatenate observations in the window\n",
    "                         obs2=ow_obs2.reshape([batch_size, -1]),  # concatenate observations in the window\n",
    "                         act=self.act_buf[idxs],\n",
    "                         rew=self.rew_buf[idxs],\n",
    "                         done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 5\n",
    "act_dim = 6\n",
    "replay_size = 10000\n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "o = np.random.randn(obs_dim)\n",
    "for i in range(100):  \n",
    "    a = np.random.randn(act_dim)\n",
    "    o2 = np.random.randn(obs_dim)\n",
    "    r = np.random.randn(1)[0]\n",
    "    if i % 3 == 0:\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "    replay_buffer.store(o, a, r, o2, d)\n",
    "    o = o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-76-bd384b530529>\u001b[0m(80)\u001b[0;36msample_batch_ow\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     78 \u001b[1;33m        \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     79 \u001b[1;33m        \u001b[1;31m# Construct batch data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 80 \u001b[1;33m        \u001b[1;32mif\u001b[0m \u001b[0madd_past_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     81 \u001b[1;33m            \u001b[1;31m# Combine past action and observation within the window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     82 \u001b[1;33m            \u001b[0mcomb_past_act_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_dim\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation_window_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ow_obs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.63720018, -1.11447906, -0.97945368,  0.05561435,\n",
      "          0.65352297],\n",
      "        [ 0.71371633, -0.81165254, -1.26463163, -0.2870765 ,\n",
      "         -1.49714816]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.83934629,  0.03621995,  0.55556411, -1.04745173,\n",
      "          1.3372339 ]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [-1.81603861, -0.50210315,  0.85174698,  1.5792253 ,\n",
      "          0.92575312],\n",
      "        [ 1.45639837, -0.1432569 ,  0.43294254,  0.57249999,\n",
      "          0.48297298]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.00281845,  0.30203035,  0.31653476, -0.21194811,\n",
      "          1.20829225],\n",
      "        [ 2.03994775, -0.41822904,  1.0218451 , -0.37317869,\n",
      "         -1.172176  ]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ],\n",
      "        [ 1.31179035, -1.38642728, -0.42949358, -0.95115775,\n",
      "          0.93101048]]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ow_act\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [-1.43707204,  0.4029538 , -0.44403645, -0.24694937,\n",
      "          0.63258284,  1.08678091]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.9131723 ,  0.81497729,  0.54666859, -1.47213233,\n",
      "          1.12315786,  2.06768656]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.09171429, -0.19171932, -0.54002148,  0.33495295,\n",
      "          1.09566283, -0.07988471]],\n",
      "\n",
      "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ]]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-76-bd384b530529>\u001b[0m(93)\u001b[0;36msample_batch_ow\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     91 \u001b[1;33m            \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     92 \u001b[1;33m            \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 93 \u001b[1;33m            batch = dict(obs=comb_past_act_obs,\n",
      "\u001b[0m\u001b[1;32m     94 \u001b[1;33m                         \u001b[0mobs2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomb_past_act_obs2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     95 \u001b[1;33m                         \u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_buf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  comb_past_act_obs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.63720018, -1.11447906,\n",
      "        -0.97945368,  0.05561435,  0.65352297, -1.43707204,  0.4029538 ,\n",
      "        -0.44403645, -0.24694937,  0.63258284,  1.08678091,  0.71371633,\n",
      "        -0.81165254, -1.26463163, -0.2870765 , -1.49714816],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.83934629,\n",
      "         0.03621995,  0.55556411, -1.04745173,  1.3372339 ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        , -1.81603861, -0.50210315,\n",
      "         0.85174698,  1.5792253 ,  0.92575312,  0.9131723 ,  0.81497729,\n",
      "         0.54666859, -1.47213233,  1.12315786,  2.06768656,  1.45639837,\n",
      "        -0.1432569 ,  0.43294254,  0.57249999,  0.48297298],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.00281845,  0.30203035,\n",
      "         0.31653476, -0.21194811,  1.20829225,  0.09171429, -0.19171932,\n",
      "        -0.54002148,  0.33495295,  1.09566283, -0.07988471,  2.03994775,\n",
      "        -0.41822904,  1.0218451 , -0.37317869, -1.172176  ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  1.31179035,\n",
      "        -1.38642728, -0.42949358, -0.95115775,  0.93101048]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Google Drive\\git_repos_2020\\spinningup_new\\spinup\\algos\\pytorch\\td3_ow\\td3_ow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_batch_ow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_past_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'obs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\git_repos_2020\\spinningup_new\\spinup\\algos\\pytorch\\td3_ow\\td3_ow.py\u001b[0m in \u001b[0;36msample_batch_ow\u001b[1;34m(self, batch_size, observation_window_size, add_past_action)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             batch = dict(obs=comb_past_act_obs,\n\u001b[0m\u001b[0;32m     94\u001b[0m                          \u001b[0mobs2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomb_past_act_obs2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                          \u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_buf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\git_repos_2020\\spinningup_new\\spinup\\algos\\pytorch\\td3_ow\\td3_ow.py\u001b[0m in \u001b[0;36msample_batch_ow\u001b[1;34m(self, batch_size, observation_window_size, add_past_action)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             batch = dict(obs=comb_past_act_obs,\n\u001b[0m\u001b[0;32m     94\u001b[0m                          \u001b[0mobs2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomb_past_act_obs2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                          \u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_buf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = replay_buffer.sample_batch_ow(batch_size=5, add_past_action=True)\n",
    "batch['obs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td3_ow(env_name, partially_observable=False, \n",
    "           observation_window_size=5, add_past_action=False,\n",
    "           actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,\n",
    "           steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99,\n",
    "           polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000,\n",
    "           update_after=1000, update_every=50, act_noise=0.1, target_noise=0.2,\n",
    "           noise_clip=0.5, policy_delay=2, num_test_episodes=10, max_ep_len=1000,\n",
    "           logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "    Twin Delayed Deep Deterministic Policy Gradient with Observation Window (TD3-OW)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env_name : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "\n",
    "        partially_observable:\n",
    "\n",
    "        observation_window_size:\n",
    "\n",
    "        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n",
    "            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.\n",
    "            The ``act`` method and ``pi`` module should accept batches of \n",
    "            observations as inputs, and ``q1`` and ``q2`` should accept a batch \n",
    "            of observations and a batch of actions as inputs. When called, \n",
    "            these should return:\n",
    "\n",
    "            ===========  ================  ======================================\n",
    "            Call         Output Shape      Description\n",
    "            ===========  ================  ======================================\n",
    "            ``act``      (batch, act_dim)  | Numpy array of actions for each \n",
    "                                           | observation.\n",
    "            ``pi``       (batch, act_dim)  | Tensor containing actions from policy\n",
    "                                           | given observations.\n",
    "            ``q1``       (batch,)          | Tensor containing one current estimate\n",
    "                                           | of Q* for the provided observations\n",
    "                                           | and actions. (Critical: make sure to\n",
    "                                           | flatten this!)\n",
    "            ``q2``       (batch,)          | Tensor containing the other current \n",
    "                                           | estimate of Q* for the provided observations\n",
    "                                           | and actions. (Critical: make sure to\n",
    "                                           | flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n",
    "            you provided to TD3.\n",
    "\n",
    "        seed (int): Seed for random number generators.\n",
    "\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "\n",
    "        polyak (float): Interpolation factor in polyak averaging for target \n",
    "            networks. Target networks are updated towards main networks \n",
    "            according to:\n",
    "\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n",
    "            close to 1.)\n",
    "\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "\n",
    "        update_after (int): Number of env interactions to collect before\n",
    "            starting to do gradient descent updates. Ensures replay buffer\n",
    "            is full enough for useful updates.\n",
    "\n",
    "        update_every (int): Number of env interactions that should elapse\n",
    "            between gradient descent updates. Note: Regardless of how long \n",
    "            you wait between updates, the ratio of env steps to gradient steps \n",
    "            is locked to 1.\n",
    "\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to \n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "\n",
    "        target_noise (float): Stddev for smoothing noise added to target \n",
    "            policy.\n",
    "\n",
    "        noise_clip (float): Limit for absolute value of target policy \n",
    "            smoothing noise.\n",
    "\n",
    "        policy_delay (int): Policy will only be updated once every \n",
    "            policy_delay times for each update of the Q-networks.\n",
    "\n",
    "        num_test_episodes (int): Number of episodes to test the deterministic\n",
    "            policy at the end of each epoch.\n",
    "\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Wrapper environment if using POMDP\n",
    "    if partially_observable:\n",
    "        env, test_env = POMDPWrapper(env_name), POMDPWrapper(env_name)\n",
    "    else:\n",
    "        env, test_env = gym.make(env_name), gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "    \n",
    "    # Create actor-critic module and target networks\n",
    "    ac = actor_critic(env.observation_space, observation_window_size, add_past_action, \n",
    "                      env.action_space, **ac_kwargs)\n",
    "    ac_targ = deepcopy(ac)\n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "    # List of parameters for both Q-networks (save this for convenience)\n",
    "    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "    var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q1, ac.q2])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t q1: %d, \\t q2: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up function for computing TD3 Q-losses\n",
    "    def compute_loss_q(data):\n",
    "        o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "\n",
    "        q1 = ac.q1(o,a)\n",
    "        q2 = ac.q2(o,a)\n",
    "\n",
    "        # Bellman backup for Q functions\n",
    "        with torch.no_grad():\n",
    "            pi_targ = ac_targ.pi(o2)\n",
    "\n",
    "            # Target policy smoothing\n",
    "            epsilon = torch.randn_like(pi_targ) * target_noise\n",
    "            epsilon = torch.clamp(epsilon, -noise_clip, noise_clip)\n",
    "            a2 = pi_targ + epsilon\n",
    "            a2 = torch.clamp(a2, -act_limit, act_limit)\n",
    "\n",
    "            # Target Q-values\n",
    "            q1_pi_targ = ac_targ.q1(o2, a2)\n",
    "            q2_pi_targ = ac_targ.q2(o2, a2)\n",
    "            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "            backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "        # MSE loss against Bellman backup\n",
    "        loss_q1 = ((q1 - backup)**2).mean()\n",
    "        loss_q2 = ((q2 - backup)**2).mean()\n",
    "        loss_q = loss_q1 + loss_q2\n",
    "\n",
    "        # Useful info for logging\n",
    "        loss_info = dict(Q1Vals=q1.detach().numpy(),\n",
    "                         Q2Vals=q2.detach().numpy())\n",
    "\n",
    "        return loss_q, loss_info\n",
    "\n",
    "    # Set up function for computing TD3 pi loss\n",
    "    def compute_loss_pi(data):\n",
    "        o = data['obs']\n",
    "        q1_pi = ac.q1(o, ac.pi(o))\n",
    "        return -q1_pi.mean()\n",
    "\n",
    "    # Set up optimizers for policy and q-function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    q_optimizer = Adam(q_params, lr=q_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "    logger.setup_pytorch_saver(ac)\n",
    "\n",
    "    def update(data, timer):\n",
    "        # First run one gradient descent step for Q1 and Q2\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q, loss_info = compute_loss_q(data)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # Record things\n",
    "        logger.store(LossQ=loss_q.item(), **loss_info)\n",
    "\n",
    "        # Possibly update pi and target networks\n",
    "        if timer % policy_delay == 0:\n",
    "\n",
    "            # Freeze Q-networks so you don't waste computational effort \n",
    "            # computing gradients for them during the policy learning step.\n",
    "            for p in q_params:\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # Next run one gradient descent step for pi.\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi = compute_loss_pi(data)\n",
    "            loss_pi.backward()\n",
    "            pi_optimizer.step()\n",
    "\n",
    "            # Unfreeze Q-networks so you can optimize it at next DDPG step.\n",
    "            for p in q_params:\n",
    "                p.requires_grad = True\n",
    "\n",
    "            # Record things\n",
    "            logger.store(LossPi=loss_pi.item())\n",
    "\n",
    "            # Finally, update target networks by polyak averaging.\n",
    "            with torch.no_grad():\n",
    "                for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                    # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                    # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                    p_targ.data.mul_(polyak)\n",
    "                    p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent():\n",
    "        for j in range(num_test_episodes):\n",
    "            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "            if add_past_action:\n",
    "                ow_o = np.zeros([int((observation_window_size-1)*(obs_dim+act_dim)+obs_dim)])\n",
    "                ow_o[-obs_dim:] = o\n",
    "            else:\n",
    "                ow_o = np.zeros([int(observation_window_size*obs_dim)])\n",
    "                ow_o[-obs_dim:] = o\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, d, _ = test_env.step(get_action(ow_o, 0))\n",
    "                if add_past_action:\n",
    "                    ow_o[:-(obs_dim+act_dim)] = ow_o[(obs_dim+act_dim):]\n",
    "                    ow_o[-(obs_dim+act_dim):] = np.concatenate((a, o))\n",
    "                else:\n",
    "                    ow_o[:-obs_dim] = ow_o[obs_dim:]\n",
    "                    ow_o[-obs_dim:] = o\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    if add_past_action:\n",
    "        ow_o = np.zeros([int((observation_window_size-1)*(obs_dim+act_dim)+obs_dim)])\n",
    "        ow_o[-obs_dim:] = o\n",
    "    else:\n",
    "        ow_o = np.zeros([int(observation_window_size*obs_dim)])\n",
    "        ow_o[-obs_dim:] = o\n",
    "    \n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for t in range(total_steps):\n",
    "        \n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards, \n",
    "        # use the learned policy (with some noise, via act_noise). \n",
    "        if t > start_steps:\n",
    "            a = get_action(ow_o, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update \n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "        \n",
    "        if add_past_action:\n",
    "            ow_o[:-(obs_dim+act_dim)] = ow_o[(obs_dim+act_dim):]\n",
    "            ow_o[-(obs_dim+act_dim):] = np.concatenate((a, o))\n",
    "        else:\n",
    "            ow_o[:-obs_dim] = ow_o[obs_dim:]\n",
    "            ow_o[-obs_dim:] = o\n",
    "\n",
    "        # End of trajectory handling\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "            if add_past_action:\n",
    "                ow_o = np.zeros([int((observation_window_size-1)*(obs_dim+act_dim)+obs_dim)])\n",
    "                ow_o[-obs_dim:] = o\n",
    "            else:\n",
    "                ow_o = np.zeros([int(observation_window_size*obs_dim)])\n",
    "                ow_o[-obs_dim:] = o\n",
    "\n",
    "        # Update handling\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            for j in range(update_every):\n",
    "                # batch = replay_buffer.sample_batch(batch_size)\n",
    "                batch = replay_buffer.sample_batch_ow(batch_size, observation_window_size)\n",
    "                update(data=batch, timer=j)\n",
    "\n",
    "        # End of epoch handling\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "            # Save model\n",
    "            if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "                logger.save_state({'env': env}, None)\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            test_agent()\n",
    "\n",
    "            # Log info about epoch\n",
    "            logger.log_tabular('Epoch', epoch)\n",
    "            logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('EpLen', average_only=True)\n",
    "            logger.log_tabular('TestEpLen', average_only=True)\n",
    "            logger.log_tabular('TotalEnvInteracts', t)\n",
    "            logger.log_tabular('Q1Vals', with_min_and_max=True)\n",
    "            logger.log_tabular('Q2Vals', with_min_and_max=True)\n",
    "            logger.log_tabular('LossPi', average_only=True)\n",
    "            logger.log_tabular('LossQ', average_only=True)\n",
    "            logger.log_tabular('Time', time.time()-start_time)\n",
    "            logger.dump_tabular()\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"Function used in argument parser for converting string to bool.\"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to C:\\Users\\Lingheng\\Google Drive\\git_repos_2020\\spinup_data_td3_ow\\2020-10-29_td3_ow\\2020-10-29_12-08-07-td3_ow_s0\\progress.txt\u001b[0m\n",
      "\u001b[36;1mSaving config:\n",
      "\u001b[0m\n",
      "{\n",
      "    \"ac_kwargs\":\t{\n",
      "        \"hidden_sizes\":\t[\n",
      "            256,\n",
      "            256\n",
      "        ]\n",
      "    },\n",
      "    \"act_noise\":\t0.1,\n",
      "    \"actor_critic\":\t\"MLPActorCritic\",\n",
      "    \"add_past_action\":\ttrue,\n",
      "    \"batch_size\":\t100,\n",
      "    \"env_name\":\t\"AntPyBulletEnv-v0\",\n",
      "    \"epochs\":\t200,\n",
      "    \"exp_name\":\t\"td3_ow\",\n",
      "    \"gamma\":\t0.99,\n",
      "    \"logger\":\t{\n",
      "        \"<spinup.utils.logx.EpochLogger object at 0x0000020C18DFE388>\":\t{\n",
      "            \"epoch_dict\":\t{},\n",
      "            \"exp_name\":\t\"td3_ow\",\n",
      "            \"first_row\":\ttrue,\n",
      "            \"log_current_row\":\t{},\n",
      "            \"log_headers\":\t[],\n",
      "            \"output_dir\":\t\"C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinup_data_td3_ow\\\\2020-10-29_td3_ow\\\\2020-10-29_12-08-07-td3_ow_s0\",\n",
      "            \"output_file\":\t{\n",
      "                \"<_io.TextIOWrapper name='C:\\\\\\\\Users\\\\\\\\Lingheng\\\\\\\\Google Drive\\\\\\\\git_repos_2020\\\\\\\\spinup_data_td3_ow\\\\\\\\2020-10-29_td3_ow\\\\\\\\2020-10-29_12-08-07-td3_ow_s0\\\\\\\\progress.txt' mode='w' encoding='cp1252'>\":\t{\n",
      "                    \"mode\":\t\"w\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"logger_kwargs\":\t{\n",
      "        \"exp_name\":\t\"td3_ow\",\n",
      "        \"output_dir\":\t\"C:\\\\Users\\\\Lingheng\\\\Google Drive\\\\git_repos_2020\\\\spinup_data_td3_ow\\\\2020-10-29_td3_ow\\\\2020-10-29_12-08-07-td3_ow_s0\"\n",
      "    },\n",
      "    \"max_ep_len\":\t1000,\n",
      "    \"noise_clip\":\t0.5,\n",
      "    \"num_test_episodes\":\t10,\n",
      "    \"observation_window_size\":\t5,\n",
      "    \"partially_observable\":\ttrue,\n",
      "    \"pi_lr\":\t0.001,\n",
      "    \"policy_delay\":\t2,\n",
      "    \"polyak\":\t0.995,\n",
      "    \"q_lr\":\t0.001,\n",
      "    \"replay_size\":\t1000000,\n",
      "    \"save_freq\":\t1,\n",
      "    \"seed\":\t0,\n",
      "    \"start_steps\":\t10000,\n",
      "    \"steps_per_epoch\":\t4000,\n",
      "    \"target_noise\":\t0.2,\n",
      "    \"update_after\":\t1000,\n",
      "    \"update_every\":\t50\n",
      "}\n",
      "WalkerBase::__init__\n",
      "WalkerBase::__init__\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 108296, \t q1: 108545, \t q2: 108545\n",
      "\u001b[0m\n",
      "options= \n",
      "> \u001b[1;32m<ipython-input-4-71ebcc510bb8>\u001b[0m(306)\u001b[0;36mtd3_ow\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    304 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    305 \u001b[1;33m        \u001b[1;31m# End of trajectory handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 306 \u001b[1;33m        \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mep_len\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_ep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    307 \u001b[1;33m            \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpRet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpLen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    308 \u001b[1;33m            \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  obs_dim+act_dim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ow_o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        2.52382301e-06,  1.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
      "       -3.14379595e-02,  0.00000000e+00, -1.87456334e+00,  0.00000000e+00,\n",
      "        1.03546470e-01,  0.00000000e+00,  1.74668431e+00,  0.00000000e+00,\n",
      "        5.10309078e-02,  0.00000000e+00,  1.71413374e+00,  0.00000000e+00,\n",
      "        7.77257383e-02,  0.00000000e+00, -1.81827950e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "       -1.96632266e-01, -7.32205212e-01, -7.01017797e-01, -6.37438655e-01,\n",
      "        2.21126050e-01,  7.57116675e-01,  8.52509320e-01,  4.47582096e-01,\n",
      "       -1.19296112e-03,  1.87523093e-03,  9.99998271e-01,  9.34333890e-04,\n",
      "        1.02932472e-03, -2.99918670e-02,  9.76065733e-03, -1.87456334e+00,\n",
      "        0.00000000e+00,  9.83967558e-02, -3.48978899e-02,  1.72778440e+00,\n",
      "       -1.11943722e-01,  5.77469394e-02,  4.54472192e-02,  1.71426070e+00,\n",
      "        4.70688130e-04,  9.22099948e-02,  9.80236828e-02, -1.81107092e+00,\n",
      "        4.27143686e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Google Drive\\git_repos_2020\\spinningup_new\\spinup\\algos\\pytorch\\td3_ow\\td3_ow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m        \u001b[0mac_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'l'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m        \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m        logger_kwargs=logger_kwargs)\n\u001b[0m",
      "\u001b[1;32m~\\Google Drive\\git_repos_2020\\spinningup_new\\spinup\\algos\\pytorch\\td3_ow\\td3_ow.py\u001b[0m in \u001b[0;36mtd3_ow\u001b[1;34m(env_name, partially_observable, observation_window_size, add_past_action, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, replay_size, gamma, polyak, pi_lr, q_lr, batch_size, start_steps, update_after, update_every, act_noise, target_noise, noise_clip, policy_delay, num_test_episodes, max_ep_len, logger_kwargs, save_freq)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# End of trajectory handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mep_len\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_ep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpRet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpLen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\git_repos_2020\\spinningup_new\\spinup\\algos\\pytorch\\td3_ow\\td3_ow.py\u001b[0m in \u001b[0;36mtd3_ow\u001b[1;34m(env_name, partially_observable, observation_window_size, add_past_action, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, replay_size, gamma, polyak, pi_lr, q_lr, batch_size, start_steps, update_after, update_every, act_noise, target_noise, noise_clip, policy_delay, num_test_episodes, max_ep_len, logger_kwargs, save_freq)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# End of trajectory handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mep_len\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_ep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpRet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpLen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\app_cloud\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "#     parser.add_argument('--partially_observable', type=str2bool, nargs='?', const=True, default=False, help=\"Using POMDP\")\n",
    "#     parser.add_argument('--hid', type=int, default=256)\n",
    "#     parser.add_argument('--l', type=int, default=2)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "#     parser.add_argument('--epochs', type=int, default=50)\n",
    "#     parser.add_argument('--exp_name', type=str, default='td3_ow')\n",
    "#     parser.add_argument(\"--data_dir\", type=str, default='spinup_data_td3_ow')\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "args = {'env':'AntPyBulletEnv-v0', \n",
    "        'partially_observable': True, 'add_past_action': True,\n",
    "        'hid': 256, 'l': 2, 'gamma': 0.99, 'seed': 0, 'epochs': 200,\n",
    "        'exp_name': 'td3_ow', 'data_dir': 'spinup_data_td3_ow'}\n",
    "__file__ = './td3_ow.py'\n",
    "\n",
    "# Set log data saving directory\n",
    "from spinup.utils.run_utils import setup_logger_kwargs\n",
    "\n",
    "data_dir = osp.join(\n",
    "    osp.dirname(osp.dirname(osp.dirname(osp.dirname(osp.dirname(osp.dirname(osp.abspath(__file__))))))),\n",
    "    args['data_dir'])\n",
    "logger_kwargs = setup_logger_kwargs(args['exp_name'], args['seed'], data_dir, datestamp=True)\n",
    "\n",
    "td3_ow(env_name=args['env'], \n",
    "       partially_observable=args['partially_observable'],\n",
    "       add_past_action=args['add_past_action'],\n",
    "       actor_critic=core.MLPActorCritic,\n",
    "       ac_kwargs=dict(hidden_sizes=[args['hid']]*args['l']),\n",
    "       gamma=args['gamma'], seed=args['seed'], epochs=args['epochs'],\n",
    "       logger_kwargs=logger_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
