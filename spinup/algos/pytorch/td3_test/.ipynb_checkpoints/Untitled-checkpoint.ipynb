{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes = [256, 256]):\n",
    "        super(MLPCritic, self).__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.layer_sizes = [obs_dim+act_dim] + hidden_sizes + [1]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Hidden layers\n",
    "        for h_i in range(len(self.layer_sizes)-2):\n",
    "            self.layers += [nn.Linear(self.layer_sizes[h_i], self.layer_sizes[h_i+1]),\n",
    "                            nn.Sigmoid()]\n",
    "        # Output layer\n",
    "        self.layers += [nn.Linear(self.layer_sizes[-2], self.layer_sizes[-1]),\n",
    "                        nn.ReLU()]\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        hidden_output = []\n",
    "        hid_activation = []\n",
    "        # Hidden layers\n",
    "        for h_i in range(len(self.layers)-1):\n",
    "            x = self.layers[h_i](x)\n",
    "            # Store activation\n",
    "            if h_i % 2 == 1:\n",
    "                print(h_i)\n",
    "                hid_activation.append(x)\n",
    "        return x, hid_activation\n",
    "\n",
    "    \n",
    "class MLPActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit, hidden_sizes=[256, 256]):\n",
    "        super(MLPActor, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.act_limit = act_limit\n",
    "        self.layer_sizes = [obs_dim] + hidden_sizes + [act_dim]\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        # Hidden layers\n",
    "        for h_i in range(len(self.layer_sizes)-2):\n",
    "            self.layers += [nn.Linear(self.layer_sizes[h_i], self.layer_sizes[h_i+1]),\n",
    "                            nn.Sigmoid()]\n",
    "        # Output layer\n",
    "        self.layers += [nn.Linear(self.layer_sizes[-2], self.layer_sizes[-1]),\n",
    "                        nn.Tanh()]\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = obs\n",
    "        hidden_output = []\n",
    "        hid_activation = []\n",
    "        # Hidden layers\n",
    "        for h_i in range(len(self.layers)-1):\n",
    "            x = self.layers[h_i](x)\n",
    "            # Store activation\n",
    "            if h_i % 2 == 1:\n",
    "                print(h_i)\n",
    "                hid_activation.append(x)\n",
    "        return self.act_limit * x, hid_activation\n",
    "    \n",
    "    \n",
    "class MLPActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit, critic_hidden_sizes=[256, 256], actor_hidden_sizes=[256, 256]):\n",
    "        super(MLPActorCritic, self).__init__()\n",
    "        self.q1 = MLPCritic(obs_dim, act_dim, critic_hidden_sizes)\n",
    "        self.q2 = MLPCritic(obs_dim, act_dim, critic_hidden_sizes)\n",
    "        self.pi = MLPActor(obs_dim, act_dim, act_limit, actor_hidden_sizes)\n",
    "        \n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            a, _ = self.pi(obs)\n",
    "            return a.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 15\n",
    "act_dim = 5\n",
    "critic = MLPCritic(obs_dim, act_dim)\n",
    "actor = MLPActor(obs_dim, act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "m = 100\n",
    "obs = torch.as_tensor(np.random.rand(m, obs_dim), dtype=torch.float32)\n",
    "act = torch.as_tensor(np.random.rand(m, act_dim), dtype=torch.float32)\n",
    "q, q_hid_activation = critic(obs, act)\n",
    "a, a_hid_activation = actor(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6067, 0.5047, 0.4744, 0.4898, 0.5349, 0.5409, 0.4726, 0.5370, 0.4718,\n",
       "        0.3470, 0.3877, 0.5362, 0.3976, 0.5017, 0.3958, 0.5525, 0.5401, 0.3854,\n",
       "        0.5290, 0.5130, 0.4945, 0.4786, 0.5506, 0.4881, 0.5183, 0.3692, 0.6020,\n",
       "        0.3939, 0.5712, 0.5318, 0.6325, 0.5938, 0.4832, 0.5813, 0.3648, 0.4099,\n",
       "        0.5243, 0.5920, 0.5059, 0.5398, 0.3487, 0.5012, 0.5444, 0.4800, 0.6469,\n",
       "        0.4534, 0.4393, 0.5691, 0.4726, 0.5828, 0.4657, 0.5383, 0.4352, 0.3629,\n",
       "        0.5125, 0.6026, 0.4704, 0.5292, 0.4727, 0.4528, 0.4770, 0.5008, 0.4778,\n",
       "        0.5587, 0.5814, 0.5431, 0.4608, 0.6139, 0.6130, 0.4333, 0.4183, 0.3449,\n",
       "        0.6047, 0.3676, 0.5674, 0.5013, 0.5519, 0.5632, 0.4392, 0.3591, 0.4591,\n",
       "        0.5530, 0.6337, 0.3952, 0.4739, 0.4734, 0.4242, 0.4878, 0.5078, 0.6197,\n",
       "        0.4293, 0.6240, 0.4365, 0.4395, 0.5399, 0.5173, 0.6532, 0.5311, 0.3310,\n",
       "        0.5044, 0.5077, 0.3787, 0.4826, 0.5108, 0.5040, 0.4619, 0.4056, 0.5470,\n",
       "        0.5123, 0.4193, 0.4274, 0.5585, 0.5530, 0.5415, 0.4294, 0.5250, 0.4798,\n",
       "        0.4987, 0.4396, 0.5413, 0.5385, 0.3953, 0.4721, 0.4219, 0.4761, 0.5150,\n",
       "        0.3520, 0.4526, 0.5221, 0.3755, 0.5088, 0.4925, 0.5041, 0.4216, 0.4788,\n",
       "        0.4176, 0.5328, 0.4441, 0.5055, 0.4924, 0.4618, 0.5756, 0.3984, 0.5846,\n",
       "        0.4427, 0.5323, 0.4980, 0.6288, 0.5909, 0.4109, 0.4983, 0.5633, 0.5660,\n",
       "        0.6074, 0.5506, 0.6260, 0.5284, 0.5682, 0.4602, 0.5431, 0.4443, 0.6455,\n",
       "        0.4491, 0.4096, 0.3754, 0.5784, 0.4714, 0.4200, 0.4550, 0.4452, 0.4208,\n",
       "        0.5103, 0.4895, 0.5513, 0.5558, 0.3808, 0.4410, 0.4900, 0.5292, 0.5107,\n",
       "        0.4629, 0.4300, 0.4647, 0.6977, 0.4475, 0.3779, 0.5011, 0.5194, 0.4926,\n",
       "        0.4154, 0.4982, 0.5212, 0.5621, 0.5026, 0.5621, 0.5496, 0.4991, 0.6226,\n",
       "        0.4655, 0.4550, 0.5231, 0.5418, 0.3942, 0.4190, 0.4918, 0.4472, 0.5137,\n",
       "        0.4124, 0.3839, 0.4899, 0.5159, 0.5607, 0.4419, 0.5665, 0.6094, 0.5040,\n",
       "        0.4541, 0.4231, 0.3712, 0.4987, 0.4766, 0.3658, 0.4791, 0.5721, 0.4626,\n",
       "        0.5343, 0.4637, 0.5998, 0.6331, 0.7062, 0.5070, 0.5296, 0.4500, 0.4036,\n",
       "        0.5226, 0.4373, 0.3884, 0.5283, 0.4427, 0.3896, 0.4418, 0.4583, 0.5276,\n",
       "        0.4409, 0.5701, 0.5850, 0.4842, 0.4923, 0.3989, 0.5400, 0.4717, 0.4866,\n",
       "        0.5631, 0.6476, 0.5651, 0.4146], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hid_a = hid_activation[0]\n",
    "hid_a.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = torch.as_tensor(0.05, dtype=torch.float32)\n",
    "beta = 0.5\n",
    "sparsity_penalty = torch.nn.functional.kl_div(torch.cat(hid_activation, dim=1).mean(axis=0), rho, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-89.3844, grad_fn=<KlDivBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
